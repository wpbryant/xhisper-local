# xhisper configuration
# Copy to ~/.config/xhisper/xhisperrc and customize
# When in doubt, check by running 'xhisper --log'

# Whisper Model Settings:
# Available models: tiny, base, small, medium, large-v3
# - tiny: fastest, least accurate (~1GB RAM)
# - base: fast, decent accuracy (~1GB RAM) [recommended]
# - small: slower, better accuracy (~2GB RAM)
# - medium: much slower, very good accuracy (~5GB RAM)
# - large-v3: slowest, best accuracy (~10GB RAM)
model-name : base

# Device: auto, cpu, or cuda
# - auto: automatically detects and uses CUDA if available
# - cpu: force CPU usage
# - cuda: force GPU (CUDA) usage
model-device : auto

# Language: leave empty for auto-detect, or specify code (e.g., en, es, fr)
# Specifying language makes transcription faster and more accurate
model-language : ""

# Transcription Settings:
transcription-prompt     : ""

# Paste Timing (seconds):
non-ascii-initial-delay : 0.15 # Increase this if first character comes out wrong.
non-ascii-default-delay : 0.025

# Silence Detection:
silence-threshold  : -50
silence-percentage : 95

# AI Post-Processing (optional):
# Improve grammar, punctuation, and capitalization using local LLM via Ollama
# - post-process-model : Ollama model name (e.g., gemma3:4b, phi3, llama3.2:3b)
#   Leave empty to disable post-processing
# - post-process-timeout : Maximum seconds to wait for LLM response
# - post-process-mode : auto, standard, command, email
#   auto: detects context based on content (default)
#   standard: normal grammar/punctuation fixes
#   command: preserves command syntax, fixes command names
#   email: formats as email with salutations and sign-offs
post-process-model     : ""
post-process-timeout   : 10
post-process-mode      : auto
